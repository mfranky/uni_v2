{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pr5 - Lineare Modelle_bl.ipynb","provenance":[{"file_id":"1-eFdEpcJYV3QVqQQeg0xto4xT2tjN7Yw","timestamp":1605095191512}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5unJaTc5-zNz"},"source":["Dieses Notebook ist angelehnt an das Buch *Python Data Science Handbook* von Jake VanderPlas, auch verfügbar auf [GitHubPages](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic)."]},{"cell_type":"code","metadata":{"id":"ewBdG2KK-zN2"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zHXzBCTP-zN6"},"source":["# Praktikum: Session 5\n","[Video](https://mstream.hm.edu/paella/ui/watch.html?id=bcb4dac3-6114-475e-a5ef-436926cf0497)\n","\n","In dieser Session stehen zwei Punkte im Vordergrund:\n","\n","1.   Datenaufbereitung\n","2.   Interpretation von Modellen\n","\n","Um den zweiten Punkt gut abbilden zu können, verwenden wir ein lineares Modell.\n","\n","Als Anwendungsfall betrachten wir die Anzahl der Fahrradfahrten über die Fremont Bridge in Seattle (dort sind entsprechende Sensoren installiert und die Daten seit Oktober 2012 verfügbar). Ziel ist es, diese Anzahl der Fahrten für einen gegebenen Tag vorherzusagen. Zu diesem Zweck wollen wir ein lineares Modell erstellen."]},{"cell_type":"markdown","metadata":{"id":"uV0bGAmv-zN7"},"source":["## 1. (Roh-)Daten\n","Die Rohdaten der Fremont Bridge können einfach heruntergeladen werden:"]},{"cell_type":"code","metadata":{"id":"ZPSw1SUY-zN8","scrolled":true,"executionInfo":{"elapsed":3736,"status":"ok","timestamp":1605095338202,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"},"user_tz":-60},"outputId":"783ff63f-705c-4aaa-b5b2-071d07a15ffa","colab":{"base_uri":"https://localhost:8080/"}},"source":["!curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 15536    0 15536    0     0  12871      0 --:--:--  0:00:01 --:--:-- 12860\n","100 1742k    0 1742k    0     0   812k      0 --:--:--  0:00:02 --:--:--  812k\n","100 3550k    0 3550k    0     0  1106k      0 --:--:--  0:00:03 --:--:-- 1106k\n","100 4248k    0 4248k    0     0  1134k      0 --:--:--  0:00:03 --:--:-- 1133k\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"f_M92uF5-zOB"},"source":["import pandas as pd\n","counts = pd.read_csv('/content/FremontBridge.csv', index_col='Date', parse_dates=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GZf8EmN0i1M","executionInfo":{"status":"ok","timestamp":1605173918460,"user_tz":-60,"elapsed":563,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}},"outputId":"9bf4bb71-49c9-4775-d46d-38db4dc202cb","colab":{"base_uri":"https://localhost:8080/","height":440}},"source":["counts"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fremont Bridge Total</th>\n","      <th>Fremont Bridge East Sidewalk</th>\n","      <th>Fremont Bridge West Sidewalk</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2012-10-03 00:00:00</th>\n","      <td>13.0</td>\n","      <td>4.0</td>\n","      <td>9.0</td>\n","    </tr>\n","    <tr>\n","      <th>2012-10-03 01:00:00</th>\n","      <td>10.0</td>\n","      <td>4.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>2012-10-03 02:00:00</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2012-10-03 03:00:00</th>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2012-10-03 04:00:00</th>\n","      <td>7.0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2020-09-30 19:00:00</th>\n","      <td>156.0</td>\n","      <td>51.0</td>\n","      <td>105.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-09-30 20:00:00</th>\n","      <td>70.0</td>\n","      <td>27.0</td>\n","      <td>43.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-09-30 21:00:00</th>\n","      <td>40.0</td>\n","      <td>17.0</td>\n","      <td>23.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-09-30 22:00:00</th>\n","      <td>23.0</td>\n","      <td>10.0</td>\n","      <td>13.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-09-30 23:00:00</th>\n","      <td>17.0</td>\n","      <td>4.0</td>\n","      <td>13.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>136334 rows × 3 columns</p>\n","</div>"],"text/plain":["                     Fremont Bridge Total  ...  Fremont Bridge West Sidewalk\n","Date                                       ...                              \n","2012-10-03 00:00:00                  13.0  ...                           9.0\n","2012-10-03 01:00:00                  10.0  ...                           6.0\n","2012-10-03 02:00:00                   2.0  ...                           1.0\n","2012-10-03 03:00:00                   5.0  ...                           3.0\n","2012-10-03 04:00:00                   7.0  ...                           1.0\n","...                                   ...  ...                           ...\n","2020-09-30 19:00:00                 156.0  ...                         105.0\n","2020-09-30 20:00:00                  70.0  ...                          43.0\n","2020-09-30 21:00:00                  40.0  ...                          23.0\n","2020-09-30 22:00:00                  23.0  ...                          13.0\n","2020-09-30 23:00:00                  17.0  ...                          13.0\n","\n","[136334 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"h35uuxHYm3Xm"},"source":["### 0. Säuberung der Daten\n","Leider enthält dieser Datensatz (Stand 11.11.2020) einen Großteil der Daten doppelt:"]},{"cell_type":"code","metadata":{"id":"bWsRkxWdnJUS","executionInfo":{"status":"ok","timestamp":1605174076450,"user_tz":-60,"elapsed":605,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}},"outputId":"fb5cf719-39a9-4810-b9c9-cae85057e33c","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(\"counts enthält %d Einträge.\" % counts.index.shape)\n","print(\"Es gibt aber nur %d einzigartige Zeitstempel.\" % counts.index.unique().shape)\n","print(\"Hier ein Auszug aus der Häufigkeitsverteilung:\")\n","counts.index.value_counts()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["counts enthält 136334 Einträge.\n","Es gibt aber nur 70080 einzigartige Zeitstempel.\n","Hier ein Auszug aus der Häufigkeitsverteilung:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["2019-06-15 03:00:00    2\n","2016-02-18 09:00:00    2\n","2018-03-21 17:00:00    2\n","2013-08-26 17:00:00    2\n","2019-05-21 16:00:00    2\n","                      ..\n","2020-07-10 08:00:00    1\n","2020-06-08 13:00:00    1\n","2020-09-12 21:00:00    1\n","2020-03-30 14:00:00    1\n","2020-03-29 15:00:00    1\n","Name: Date, Length: 70080, dtype: int64"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"5fxHQrbIvGSi"},"source":["Bevor wir diese entfernen, sollten wir prüfen, ob es fehlende Daten gibt. Diese sollten ggf. entfernt werden, **bevor** doppelte Zeilen gelöscht werden (*warum?*)."]},{"cell_type":"code","metadata":{"id":"hR3BIs7UvGSj","executionInfo":{"status":"ok","timestamp":1605174391969,"user_tz":-60,"elapsed":603,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}},"outputId":"9f4f7765-4466-4267-a2e2-d4dfa3a2766c","colab":{"base_uri":"https://localhost:8080/","height":714}},"source":["counts[counts.isna().any(axis=1)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fremont Bridge Total</th>\n","      <th>Fremont Bridge East Sidewalk</th>\n","      <th>Fremont Bridge West Sidewalk</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2013-06-14 09:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 10:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-03-09 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-03-08 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 11:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 12:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2016-03-13 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2017-03-12 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2018-03-11 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2019-03-10 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 09:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 10:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-03-09 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-03-08 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 11:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 12:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2016-03-13 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2017-03-12 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2018-03-11 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2019-03-10 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2020-03-08 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     Fremont Bridge Total  ...  Fremont Bridge West Sidewalk\n","Date                                       ...                              \n","2013-06-14 09:00:00                   NaN  ...                           NaN\n","2013-06-14 10:00:00                   NaN  ...                           NaN\n","2014-03-09 02:00:00                   NaN  ...                           NaN\n","2015-03-08 02:00:00                   NaN  ...                           NaN\n","2015-04-21 11:00:00                   NaN  ...                           NaN\n","2015-04-21 12:00:00                   NaN  ...                           NaN\n","2016-03-13 02:00:00                   NaN  ...                           NaN\n","2017-03-12 02:00:00                   NaN  ...                           NaN\n","2018-03-11 02:00:00                   NaN  ...                           NaN\n","2019-03-10 02:00:00                   NaN  ...                           NaN\n","2013-06-14 09:00:00                   NaN  ...                           NaN\n","2013-06-14 10:00:00                   NaN  ...                           NaN\n","2014-03-09 02:00:00                   NaN  ...                           NaN\n","2015-03-08 02:00:00                   NaN  ...                           NaN\n","2015-04-21 11:00:00                   NaN  ...                           NaN\n","2015-04-21 12:00:00                   NaN  ...                           NaN\n","2016-03-13 02:00:00                   NaN  ...                           NaN\n","2017-03-12 02:00:00                   NaN  ...                           NaN\n","2018-03-11 02:00:00                   NaN  ...                           NaN\n","2019-03-10 02:00:00                   NaN  ...                           NaN\n","2020-03-08 02:00:00                   NaN  ...                           NaN\n","\n","[21 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"fjAu51CivGSl"},"source":["Es gibt also 21 Zeilen, die fehlende Daten enthalten. Da wir wissen, dass es Doppelungen in den Indices gibt, besteht die Chance, dass die hier fehlenden Daten in der gedoppelten Zeile eingetragen sind. Prüfen wir das:"]},{"cell_type":"code","metadata":{"id":"Xm8ameeyvGSm","executionInfo":{"status":"ok","timestamp":1605174545377,"user_tz":-60,"elapsed":525,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}},"outputId":"40e9e184-92e3-4bb1-d6cc-1070aa68c9c6","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["counts.loc[counts[counts.isna().any(axis=1)].index]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fremont Bridge Total</th>\n","      <th>Fremont Bridge East Sidewalk</th>\n","      <th>Fremont Bridge West Sidewalk</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2013-06-14 09:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 09:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 10:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 10:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-03-09 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-03-09 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-03-08 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-03-08 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 11:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 11:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 12:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 12:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2016-03-13 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2016-03-13 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2017-03-12 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2017-03-12 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2018-03-11 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2018-03-11 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2019-03-10 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2019-03-10 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 09:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 09:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 10:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2013-06-14 10:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-03-09 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-03-09 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-03-08 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-03-08 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 11:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 11:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 12:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2015-04-21 12:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2016-03-13 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2016-03-13 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2017-03-12 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2017-03-12 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2018-03-11 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2018-03-11 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2019-03-10 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2019-03-10 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2020-03-08 02:00:00</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-03-08 02:00:00</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     Fremont Bridge Total  ...  Fremont Bridge West Sidewalk\n","Date                                       ...                              \n","2013-06-14 09:00:00                   NaN  ...                           NaN\n","2013-06-14 09:00:00                   NaN  ...                           NaN\n","2013-06-14 10:00:00                   NaN  ...                           NaN\n","2013-06-14 10:00:00                   NaN  ...                           NaN\n","2014-03-09 02:00:00                   NaN  ...                           NaN\n","2014-03-09 02:00:00                   NaN  ...                           NaN\n","2015-03-08 02:00:00                   NaN  ...                           NaN\n","2015-03-08 02:00:00                   NaN  ...                           NaN\n","2015-04-21 11:00:00                   NaN  ...                           NaN\n","2015-04-21 11:00:00                   NaN  ...                           NaN\n","2015-04-21 12:00:00                   NaN  ...                           NaN\n","2015-04-21 12:00:00                   NaN  ...                           NaN\n","2016-03-13 02:00:00                   NaN  ...                           NaN\n","2016-03-13 02:00:00                   NaN  ...                           NaN\n","2017-03-12 02:00:00                   NaN  ...                           NaN\n","2017-03-12 02:00:00                   NaN  ...                           NaN\n","2018-03-11 02:00:00                   NaN  ...                           NaN\n","2018-03-11 02:00:00                   NaN  ...                           NaN\n","2019-03-10 02:00:00                   NaN  ...                           NaN\n","2019-03-10 02:00:00                   NaN  ...                           NaN\n","2013-06-14 09:00:00                   NaN  ...                           NaN\n","2013-06-14 09:00:00                   NaN  ...                           NaN\n","2013-06-14 10:00:00                   NaN  ...                           NaN\n","2013-06-14 10:00:00                   NaN  ...                           NaN\n","2014-03-09 02:00:00                   NaN  ...                           NaN\n","2014-03-09 02:00:00                   NaN  ...                           NaN\n","2015-03-08 02:00:00                   NaN  ...                           NaN\n","2015-03-08 02:00:00                   NaN  ...                           NaN\n","2015-04-21 11:00:00                   NaN  ...                           NaN\n","2015-04-21 11:00:00                   NaN  ...                           NaN\n","2015-04-21 12:00:00                   NaN  ...                           NaN\n","2015-04-21 12:00:00                   NaN  ...                           NaN\n","2016-03-13 02:00:00                   NaN  ...                           NaN\n","2016-03-13 02:00:00                   NaN  ...                           NaN\n","2017-03-12 02:00:00                   NaN  ...                           NaN\n","2017-03-12 02:00:00                   NaN  ...                           NaN\n","2018-03-11 02:00:00                   NaN  ...                           NaN\n","2018-03-11 02:00:00                   NaN  ...                           NaN\n","2019-03-10 02:00:00                   NaN  ...                           NaN\n","2019-03-10 02:00:00                   NaN  ...                           NaN\n","2020-03-08 02:00:00                   0.0  ...                           0.0\n","2020-03-08 02:00:00                   NaN  ...                           NaN\n","\n","[42 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"SgXKjHoSvGSo"},"source":["Eine Korrektur rentiert sich hier wohl eher nicht... Daher löschen wir sie:"]},{"cell_type":"code","metadata":{"id":"-DcBmsI5vGSo"},"source":["counts.dropna(inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKQKSrbBvGSq"},"source":["Nun wollen wir diejenigen Zeilen, der Index (also der Zeitstempel) doppelt auftreten löschen; genauer: wir wollen die Doppelungen löschen, die jeweils erste Instanz soll beibehalten werden.\n","\n","Mit ``counts.index`` können wir auf den Index zugreifen. Die Methode ``duplicated()`` liefert uns Informationen darüber, ab es sich jeweils um ein Duplikat handelt (``True``) oder nicht (``False``). Die jeweils erste Instanz wird dabei nicht als Duplikat markiert. Durch den Operator ``~`` können wir eine logische Negation durchführen und erhalten damit einen Index, der genau dort den Wert ``True`` hat, wo *kein* Duplikat steht. Die dazu korrespondierenden Zeilen wollen wir behalten."]},{"cell_type":"code","metadata":{"id":"U9Nd0O5evGSr","executionInfo":{"status":"ok","timestamp":1605174742777,"user_tz":-60,"elapsed":598,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}},"outputId":"4f1b361a-d184-498b-e589-9b0377bfbf16","colab":{"base_uri":"https://localhost:8080/"}},"source":["~counts.index.duplicated()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ True,  True,  True, ...,  True,  True,  True])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"gpAvDyZ3vGSt"},"source":["counts = counts[~counts.index.duplicated()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgBgmNMOvGSv","executionInfo":{"status":"ok","timestamp":1605174767583,"user_tz":-60,"elapsed":550,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}},"outputId":"77834a6f-c334-4384-9cba-04d70b4beecf","colab":{"base_uri":"https://localhost:8080/"}},"source":["counts.index.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70070,)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"QQnidhUe-zOI"},"source":["Offenbar sind hier die Überquerungen in einem Stundenraster vorhanden. Außerdem ist neben dem Gesamtverkehr auch noch die Aufteilung in östliche und westliche Sput vorhanden (westlich: Richtung Downtown). Wir wollen diese Daten noch abändern, denn:\n","- Wir sind nur am gesamten Verkehr interessiert.\n","- Wir interessieren uns nur für die gesamten Überquerungen eines ganzen Tages.\n","\n","Für diese für uns relevanten Informationen legen wir einen DataFrame ``daily`` an:"]},{"cell_type":"code","metadata":{"id":"r1k7Q_f9-zOJ"},"source":["daily = counts.resample('d').sum()\n","daily['Total'] = daily['Fremont Bridge Total']\n","daily = daily[['Total']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MV0kBM7y-zOM","executionInfo":{"status":"ok","timestamp":1605175031173,"user_tz":-60,"elapsed":562,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}},"outputId":"f302ef4c-0309-4209-99c2-323ead4b32be","colab":{"base_uri":"https://localhost:8080/","height":230}},"source":["daily.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Total</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2012-10-03</th>\n","      <td>3521.0</td>\n","    </tr>\n","    <tr>\n","      <th>2012-10-04</th>\n","      <td>3475.0</td>\n","    </tr>\n","    <tr>\n","      <th>2012-10-05</th>\n","      <td>3148.0</td>\n","    </tr>\n","    <tr>\n","      <th>2012-10-06</th>\n","      <td>2006.0</td>\n","    </tr>\n","    <tr>\n","      <th>2012-10-07</th>\n","      <td>2142.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Total\n","Date              \n","2012-10-03  3521.0\n","2012-10-04  3475.0\n","2012-10-05  3148.0\n","2012-10-06  2006.0\n","2012-10-07  2142.0"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"0KJH7zjl-zOQ"},"source":["Basierend darauf können wir noch kein Modell erstellen, wir werden weitere Daten brauchen.\n","- Überlegen Sie, von welchen Einflussgrößen der tägliche Fahrradverkehr abhängen könnte."]},{"cell_type":"markdown","metadata":{"id":"yrY7gcthUk1T"},"source":[""]},{"cell_type":"code","metadata":{"id":"o1fboXnpU20F","cellView":"form"},"source":["#@markdown Folgende Daten wollen wir als Features verwenden: *(Doppelklick zum Einblenden)*\n","\n","1. Wochentag\n","2. Feiertag ja/nein\n","3. Zeitpunkt im Jahr (genauer: Stunden mit Tageslicht)\n","4. Temperatur\n","5. Niederschlag\n","6. Trend von Jahr zu Jahr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WNnDd7ll-zOS"},"source":["### 1. Wochentag\n","Hierfür brauchen wir keine neuen Daten, denn die Information ist implizit mit dem Datum bereits vorhanden. Allerdings müssen wir sie extrahieren, denn sonst kann sie ein lineares Modell nicht bekommen (da nichtlinearer, periodischer Zusammenhang).\n","Wir wollen für jeden Wochentag eine eigene Spalte anlegen, die genau dann eine 1 enthält, wenn der aktuelle Tag dieser Wochentag ist, sonst soll sie 0 enthalten.\n","\n","* Hinweis: ``daily.index`` gibt nur den Index des DataFrame ``daily`` zurück. In diesem Fall besteht dieser aus kalendarischen Daten, welche direkt in Form eine ``DatetimeIndex`` vorliegen. Das ist praktisch, denn solch ein ``DatetimeIndex`` hat die Methode ``dayofweek``, d.h. ``daily.index.dayofweek`` gibt die Information über den Wochentag zurück."]},{"cell_type":"code","metadata":{"id":"yWYxXkDzFMxG","executionInfo":{"status":"ok","timestamp":1605199245042,"user_tz":-60,"elapsed":592,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"m_QOo07s-zOT","executionInfo":{"status":"ok","timestamp":1605199249319,"user_tz":-60,"elapsed":619,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-cSZNue-zOa"},"source":["### 2. Feiertage\n","Diese Information ist rein aus dem Datum noch nicht ablesbar, wir brauchen zusätzlich noch einen Feiertagskalender. Dieser wird im Folgenden eingelesen und unserem DataFrame ``daily`` hinzugefügt.\n","- Untersuchen Sie, wie dies geschieht."]},{"cell_type":"code","metadata":{"id":"pKBUZkrM-zOb"},"source":["from pandas.tseries.holiday import USFederalHolidayCalendar\n","cal = USFederalHolidayCalendar()\n","holidays = cal.holidays('2012', 'today')\n","daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n","daily['holiday'].fillna(0, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35rfBWnA-zOe"},"source":["### 3. Tageslichtstunden\n","Auch die Information, wie lang die einzelnen Tage sind (Tageslichtstunden), ist in den Daten noch nicht vorhanden. Diese kann aber aus dem Datum und der geographischen Lage *berechnet* werden. In der unten stehenden Funktion ``hours_of_daylight`` ist die astronomische Standardberechnung implementiert. Nutzen Sie diese, um dem DataFrame ``daily`` eine weitere Spalte ``daylight_hrs`` hinzuzufügen, welche diese Information enthält. Stellen Sie diese neue Spalte graphisch dar, indem Sie ``daily['daylight_hrs'].plot()`` verwenden.\n","\n","*Hinweis:* Mit dem Befehl ``map(hours_of_daylight, Daten)`` können Sie die Funktion ``hours_of_daylight`` auf Daten anwenden, die Funktion ``list(...)`` macht daraus eine Liste."]},{"cell_type":"code","metadata":{"id":"uSEs4qy5-zOh"},"source":["from datetime import datetime\n","\n","def hours_of_daylight(date, axis=23.44, latitude=47.61):\n","    \"\"\"Compute the hours of daylight for the given date\"\"\"\n","    days = (date - datetime(2000, 12, 21)).days\n","    m = (1. - np.tan(np.radians(latitude))\n","         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n","    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ps8g6BbRVR1q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06qonyus-zOv"},"source":["### 4. Temperatur / 5. Niederschlag\n","Diese Informationen sind ganz offensichtlich nicht in den Daten vorhanden und können daraus auch nicht berechnet werden. Wir müssen daher eine zweite Datenquelle verwenden. Wir verwenden die Wetterdaten der [NOAA](https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND). Damit Sie die Daten dort nicht einzeln bestellen müssen, stelle ich diese bei Moodle bereit. Importen Sie diese. \n","\n","*Hinweis:* Da wir die täglichen Wetterdaten unserer ``daily`` Tabelle hinzufügen wollen, wäre es wohl sinnvoll, wenn die Datumsinformationen aus der csv-Datei direkt aufbereitet würden (eben als Datum) und nicht als String (also als inhaltsleerer Text) vorliegen würden. ``pd.read_csv`` hat entsprechende Optionen.\n","\n","*Hinweis:* Gehen Sie analog zum Einlesen der Datei *FremontBridge.csv*, siehe oben, vor."]},{"cell_type":"code","metadata":{"id":"0_C-3zkI-zOx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHNAN5bPVWxv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fkuguaXc-zPA"},"source":["Bei der Durchschittstemperatur gibt es einige fehlende Daten (wie könnte man das herausfinden?). Legen Sie eine neue Spalte ``weather['Temp (C)']`` an, welche falls vorhanden die Durchschnittstemperatur ``TAVG`` und ansonsten als Näherung hierfür den Mittelwert aus ``TMIN`` und ``TMAX``.\n","\n","*Hinweis:* Mit ``weather.loc[Zeilen, Spalten]`` kann man (auch schreibend) auf die durch Zeilen und Spalten spezifizierten Einträge zugreifen. ``weather['Temp (C)'].isnull()`` kann für die Zeilenwahl hilfreich sein."]},{"cell_type":"code","metadata":{"id":"xIcu1F4o-zPI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fSDXEsXe-zPS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZasRrCkVZy8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ng8RW78lVZDR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkbcLTtu-zPg"},"source":["Was den Niederschlag angeht, so wäre es evtl. auch relevant explizit zu wissen, ob es an einem Tag regnet (also ``PRCP!=0``) oder eben nicht (also ``PRCP==0``). Legen Sie hierfür eine neue Spalte ``weather[dry day]`` an, die an trockenen Tagen 1 enthält und sonst 0.\n","\n","*Hinweis:* Ein Vergleich wie ``PRCP==0`` liefert einen Wahrheitswert, d.h. ``True`` oder ``False`` zurück. Damit kann unser Modell aber nicht rechnen. Ein nachgestelltes ``.astype(int)`` sorgt dafür, dass ``True`` als 1 und ``False`` als 0 angegeben wird."]},{"cell_type":"code","metadata":{"id":"Av_X9Mu4-zPi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLJif74c-zPn"},"source":["Diese Wetterdaten (also ``'PRCP', 'Temp (C)', 'dry day'``) sollen nun den ``daily`` Daten hinzugefügt werden. Hierfür gibt es die Methode ``join``: Durch den Befehl ``df1.join(df2)`` wird ein neuer DataFrame erstellt, welcher die Daten aus ``df1`` und die Daten aus ``df2`` zusammengefügt enthält. Dabei sollten die Indices der beiden DataFrames übereinstimmen."]},{"cell_type":"code","metadata":{"id":"vrT3pVZm-zPo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ouidwPYf-zPr"},"source":["### 6. Trend von Jahr zu Jahr\n","Um einen etwa vorhandenen langfristigen Trend ggf. mit aufnehmen zu können, soll eine weiteres Feature angeben, wie viele Tage seit Beginn der Messung vergangen sind. Damit haben wir eine monoton steigende Größe.\n","\n","Legen Sie ein Feature ``annual`` an, das die beschriebene Größe skaliert mit 1/365 angibt. D.h. ``annual`` soll nach einem Jahr den Wert 1, nach zwei Jahren den Wert 2  usw. haben.\n","\n","*Hinweis:* Das Datum ist im Index gespeichert, auf den mit ``daily.index`` zugegriffen werden kann. ``daily.index[0]`` gibt das erste Datum als ``Timestamp`` zurück. Mit solchen Timestamps kann ganz einfach gerechnet werden, so gibt etwa die Differenz zweier Timestamps den dazwischen liegenden Zeitraum an (als ``Timedelta``-Objekt, mit ``.days`` extrahiert man daraus die Tage)."]},{"cell_type":"code","metadata":{"id":"D-Vs7_5B-zPs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwK1gT0KVddf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1txR7BmFhl4"},"source":["### 7. Pandemie\n","Man kann davon ausgehen, dass die Pandemie-Situation u.a. den Fahrradverkehr über die Fremont-Bridge signifikant beeinflusst; dies bestätigt sich auch in den Daten.\n","\n","\n","\n","1.   Formulieren Sie eine Hypothese (oder zwei...) bzgl. der Auswirkung\n","2.   Fügen Sie dem DataFrame ``daily`` eine neue Spalte ``pandemic`` hinzu, die für alle Daten ab dem 01.03.2020 den Wert 1 enthält, sonst 0.\n","\n"]},{"cell_type":"code","metadata":{"id":"Ccyw0lP9WQb_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H_ByaEMK-zPz"},"source":["Wir prüfen, ob nun noch irgendwo Werte fehlen:"]},{"cell_type":"code","metadata":{"id":"EXNvUGSQWUU-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6yuIPf04-zP3"},"source":["## 2. Modell\n","Entnehmen Sie aus ``daily`` die Features Matrix ``X`` und den Labels Vektor ``y``."]},{"cell_type":"code","metadata":{"id":"FXkBttWM-zP4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zPtb4BZc-zP6"},"source":["Trainieren Sie ein ``LinearRegression`` Modell auf die Daten. Setzen Sie den Hyperparameter ``fit_intercept=False``. Plotten Sie die erhaltenen Ergebnisse und vergleichen Sie diese (im Plot) mit den echten Daten.\n","\n","*Hinweis:* Sie können (eine Auswahl) eines DataFrames ``df`` einfach plotten durch ``df[['Item_1', ..., 'Item_N']].plot(alpha=0.5)``. Dabei sorgt ``alpha=0.5`` für einen nicht deckenden Plot, so dass sich ggf. überlagernde Linien sichtbar bleiben."]},{"cell_type":"code","metadata":{"id":"D_dYtGzh-zP7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6OZs7VbTWYZy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"664_O1dJ-zQB"},"source":["Betrachten Sie auch kleinere Zeiträume im Plot (im DataFrame ``df`` können durch ``df.loc['Startdatum':'Enddatum']`` nur die entsprechenden Daten gefiltert werden)."]},{"cell_type":"code","metadata":{"id":"j-0yfucvWaZg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T471kxAr-zQG"},"source":["Da wir ein lineares Modell verwendet haben, können wir uns die einzelnen Koeffizienten gut vorstellen; diese geben jeweils an, wie stark das entsprechende Feature in den vorhergesagten Wert eingeht. Die Koeffizienten sind nach dem Training in ``model.coef_`` verfügbar und beziehen sich der Reihe nach auf die Features. Hier eine übersichtliche tabellarische Darstellung:"]},{"cell_type":"code","metadata":{"id":"Y67DdmiuWdfX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VhAp_EA-zQJ"},"source":["- Interpretieren Sie diese Daten.\n","- Um die Güte dieser Abhängigkeiten einschätzen zu können, brauchen wir die Standardabweichungen dieser Werte. Diese erhalten wir, indem wir das Modell mehrfach (z.B. 1000 Mal) auf zufällig ausgewählten Daten trainieren. Dadurch erhalten wir für jeden Koeffizienten 1000 Werte, davon bestimmen wir die Standardabweichung."]},{"cell_type":"code","metadata":{"id":"tPXdu889-zQK"},"source":["from sklearn.utils import resample\n","np.random.seed(1)\n","err = np.std([model.fit(*resample(X, y)).coef_\n","              for i in range(1000)], 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRG9g_ON-zQM","executionInfo":{"status":"ok","timestamp":1605178429111,"user_tz":-60,"elapsed":564,"user":{"displayName":"Christian Moeller","photoUrl":"","userId":"02505173711259660088"}},"outputId":"55199782-b39b-4395-d8bd-261fd1e8dac4","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(pd.DataFrame({'effect': params.round(0),\n","                    'error': err.round(0)}))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              effect  error\n","Mon            464.0   64.0\n","Tue            559.0   66.0\n","Wed            546.0   65.0\n","Thu            417.0   65.0\n","Fri            108.0   63.0\n","Sat          -1194.0   64.0\n","Sun          -1255.0   64.0\n","holiday      -1228.0   98.0\n","daylight_hrs   120.0    7.0\n","PRCP           -27.0    2.0\n","dry day        550.0   27.0\n","Temp (C)        72.0    3.0\n","annual          83.0    5.0\n","pandemic     -1289.0   78.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VL38gWGK-zQP"},"source":["Interpretieren Sie diese Daten."]},{"cell_type":"markdown","metadata":{"id":"qh45nLC_We-M"},"source":[""]}]}